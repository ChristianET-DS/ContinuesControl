{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.32 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mjupyter-console 6.4.3 has requirement jupyter-client>=7.0.0, but you'll have jupyter-client 5.2.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "single_agent = False\n",
    "if single_agent:\n",
    "    # select this option to load version 1 (with a single agent) of the environment\n",
    "    env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "else:\n",
    "    # select this option to load version 2 (with 20 agents) of the environment\n",
    "    env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0669999985024333\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return -lim, lim\n",
    "\n",
    "\n",
    "def reset_parameters(layers):\n",
    "    for ith_layer in range(0, len(layers) - 1):\n",
    "        layer = layers[ith_layer]\n",
    "        layer.weight.data.uniform_(*hidden_init(layer))\n",
    "    layers[-1].weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\" The abstract Model \"\"\"\n",
    "    def __init__(self, name, state_size, action_size, random_seed, *args):\n",
    "        \"\"\" Initialise model parameters\n",
    "            :param name: Specifies the name of the model (for convenience)\n",
    "            :param state_size: Dimension of the state space of an environment\n",
    "            :param action_size: Dimension of the action space of an environment\n",
    "            :param random_seed: Random seed\n",
    "            :param args: Sizes of hidden layers\n",
    "         \"\"\"\n",
    "        if len(args) == 0:\n",
    "            raise ValueError(\"Hidden layer units not specified\")\n",
    "        super(Model, self).__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.name = name\n",
    "\n",
    "    \n",
    "    def forward(self, state, action=None):\n",
    "        pass\n",
    "\n",
    "    def print_(self):\n",
    "        print(\"Initialised '{}' model\".format(self.name))\n",
    "\n",
    "\n",
    "class Actor(Model):\n",
    "    def __init__(self, name, state_size, action_size, random_seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"\n",
    "        Initialise Actor model (policy gradient function)\n",
    "        :param fc1_units: Nodes in 1st hidden layer\n",
    "        :param fc2_units: Nodes in 2nd hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__(name, state_size, action_size, random_seed, fc1_units, fc2_units)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        reset_parameters([self.fc1, self.fc2, self.fc3])\n",
    "        self.print_()\n",
    "\n",
    "    def forward(self, state, action=None):\n",
    "        \"\"\" Perform forward pass and map state to action \"\"\"\n",
    "        state = F.relu(self.fc1(state))\n",
    "        state = F.relu(self.fc2(state))\n",
    "        return torch.tanh(self.fc3(state))\n",
    "\n",
    "\n",
    "class Critic(Model):\n",
    "    def __init__(self, name, state_size, action_size, random_seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"\n",
    "        Initialise Critic model (value based function)\n",
    "        :param fc1_units: Nodes in 1st hidden layer\n",
    "        :param fc2_units: Nodes in 2nd hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__(name, state_size, action_size, random_seed, fc1_units, fc2_units)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units + action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        reset_parameters([self.fc1, self.fc2, self.fc3])\n",
    "        self.print_()\n",
    "\n",
    "    def forward(self, state, action=None):\n",
    "        \"\"\" Perform forward pass and map state and action to Q values \"\"\"\n",
    "        assert action is not None, \"Action cannot be none\"\n",
    "        xs = F.leaky_relu(self.fc1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "class ActionNoise:\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OUNoise(ActionNoise):\n",
    "    \"\"\" Ornstein-Uhlenbeck exploration noise process for temporally correlated noise \"\"\"\n",
    "\n",
    "    def __init__(self, action_size, seed, mu=0., theta=.15, sigma=.2):\n",
    "        self.mu = mu * np.ones(action_size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = None\n",
    "        self.action_size = action_size\n",
    "        random.seed(seed)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.action_size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class AdaptiveNoise(ActionNoise):\n",
    "    \"\"\" Adds adaptive noise to the parameters of the neural network policy - allows for quicker training\n",
    "        For more details, see: https://openai.com/blog/better-exploration-with-parameter-noise/\n",
    "     \"\"\"\n",
    "    def __init__(self, initial_stddev=.1, desired_action_stddev=.1, adoption_coefficient=1.01):\n",
    "        self.initial_stddev = initial_stddev\n",
    "        self.desired_action_stddev = desired_action_stddev\n",
    "        self.adoption_coefficient = adoption_coefficient\n",
    "        self.current_stddev = initial_stddev\n",
    "\n",
    "    def adapt(self, distance):\n",
    "        if distance > self.desired_action_stddev:\n",
    "            self.current_stddev /= self.adoption_coefficient  # decrease standard deviation\n",
    "        else:\n",
    "            self.current_stddev *= self.adoption_coefficient  # increase standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "\n",
    "\n",
    "class Experience:\n",
    "    \"\"\" Helper class to encapsulate an experience \"\"\"\n",
    "\n",
    "    def __init__(self, state, action, reward, next_state, done):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.done = done\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_action(self):\n",
    "        return self.action\n",
    "\n",
    "    def get_reward(self):\n",
    "        return self.reward\n",
    "\n",
    "    def get_next_state(self):\n",
    "        return self.next_state\n",
    "\n",
    "    def get_done(self):\n",
    "        return self.done\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size: int, buffer_size: int, batch_size: int, random_seed: int):\n",
    "        \"\"\" Initialize a ReplayBuffer object.\n",
    "            :param buffer_size (int): maximum size of buffer\n",
    "            :param batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, experience: Experience):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(experience.get_state(), experience.get_action(), experience.get_reward(),\n",
    "                            experience.get_next_state(), experience.get_done())\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(DEVICE)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(\n",
    "            DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(\n",
    "            DEVICE)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "\n",
    "ACTOR_LR = 1e-4\n",
    "CRITIC_LR = 1e-4\n",
    "WEIGHT_DECAY = 0\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128  # minibatch size\n",
    "GAMMA = .99\n",
    "TAU = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def minimize_loss(loss, optimizer: optim.Adam, is_critic=False, critic=None):\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if is_critic and critic is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(critic.parameters(), 1)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def soft_update(local_model, target_model):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    Params\n",
    "    ======\n",
    "        local_model: PyTorch model (weights will be copied from)\n",
    "        target_model: PyTorch model (weights will be copied to)\n",
    "        tau (float): interpolation parameter\n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(TAU * local_param.data + (1.0 - TAU) * target_param.data)\n",
    "\n",
    "\n",
    "class ContinuousControlAgent:\n",
    "    \"\"\" The agent that learns to interact with an environment using the DDPG algorithm \"\"\"\n",
    "    def __init__(self, state_size, action_size, random_seed, memory: ReplayBuffer, update_frequency=10):\n",
    "        \"\"\"\n",
    "        Initialise the agent\n",
    "        :param state_size: Dimension of the state space\n",
    "        :param action_size: Dimension of the action space\n",
    "        :param random_seed: Random seed\n",
    "        \"\"\"\n",
    "        random.seed(random_seed)\n",
    "\n",
    "        self.time_step = 0\n",
    "        self.update_frequency = update_frequency\n",
    "\n",
    "        # Initialise the Actor networks (local and target), including the Optimizer\n",
    "        self.actor_local = Actor(\"Actor: Local\", state_size, action_size, random_seed).to(DEVICE)\n",
    "        self.actor_target = Actor(\"Actor: Target\", state_size, action_size, random_seed).to(DEVICE)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=ACTOR_LR)\n",
    "\n",
    "        # Initialise the Critic networks (local and target)\n",
    "        self.critic_local = Critic(\"Critic: Local\", state_size, action_size, random_seed).to(DEVICE)\n",
    "        self.critic_target = Critic(\"Critic: Target\", state_size, action_size, random_seed).to(DEVICE)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=CRITIC_LR,\n",
    "                                           weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Exploration noise process\n",
    "        self.noise = OUNoise(action_size, 0)\n",
    "\n",
    "        # Replay buffer\n",
    "        self.memory = memory\n",
    "\n",
    "        self.ready_to_learn = len(self.memory) > BATCH_SIZE\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\" Return the action for the state as per the policy \"\"\"\n",
    "        state = torch.from_numpy(state).float().to(DEVICE)\n",
    "        self.actor_local.eval()  # put the policy in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()  # put policy back in training mode\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def step(self, experience: Experience):\n",
    "        \"\"\" Add experiences to the experience buffer and learn from a batch \"\"\"\n",
    "        self.memory.add(experience)\n",
    "        if not self.ready_to_learn:\n",
    "            if len(self.memory) % (BATCH_SIZE // 4) == 0:\n",
    "                print(\"Agent experiences collected {}, agent requires  {}\"\n",
    "                      \" experiences\".format(len(self.memory), BATCH_SIZE))\n",
    "            self.ready_to_learn = len(self.memory) > BATCH_SIZE\n",
    "\n",
    "        self.time_step = (self.time_step + 1) % self.update_frequency\n",
    "        if self.time_step == 0:\n",
    "            if self.ready_to_learn:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn_(experiences)\n",
    "\n",
    "    def local_actor_network(self):\n",
    "        return self.actor_local\n",
    "\n",
    "    def learn_(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i) - Check https://arxiv.org/pdf/1509.02971.pdf paper\n",
    "        q_targets = rewards + (GAMMA * q_targets_next * (1 - dones))\n",
    "        # compute critic loss\n",
    "        q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(q_expected, q_targets)\n",
    "        minimize_loss(critic_loss, self.critic_optimizer, is_critic=True, critic=self.critic_local)\n",
    "\n",
    "        # update the actor\n",
    "        actions_predicted = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_predicted).mean()\n",
    "        minimize_loss(actor_loss, self.actor_optimizer)\n",
    "\n",
    "        # update target networks\n",
    "        soft_update(self.critic_local, self.critic_target)\n",
    "        soft_update(self.actor_local, self.actor_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_tuple(env_info):\n",
    "    \"\"\" Returns a tuple of next state, reward, and done when the agent steps through the environment based\n",
    "        on the action taken\n",
    "        :param env_info: Object holding information about the environment at a certain point\n",
    "    \"\"\"\n",
    "    if single_agent:\n",
    "        return env_info.vector_observations[0], env_info.rewards[0], env_info.local_done[0]\n",
    "    return env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ddpg(agent: ContinuousControlAgent, env: UnityEnvironment, num_episodes=2000, target=30., max_time_steps=500,\n",
    "         saved_model=\"checkpoint.pth\"):\n",
    "    \"\"\" Train an agent using the DDPG algorithm\n",
    "\n",
    "        :param agent: a continuous control agent\n",
    "        :param env: environment the agent interacts with\n",
    "        :param num_episodes: the number of episodes to train the agent\n",
    "        :param target: The average target score the agent needs to achieve for optimal performance\n",
    "        :param max_time_steps: Maximum time steps per episode\n",
    "        :param saved_model: The file path to save the model weights\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now()\n",
    "    print(now, \"- Training {}\".format(\"single agent\" if single_agent else \"{} agents\".format(20))\n",
    "          + \" for max {} episodes. Target score to reach is {}\".format(num_episodes, target))\n",
    "    # collections to help keep track of the score\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    stats = {\"scores\": [], \"episodes\": []}  # collects stats for plotting purposes\n",
    "    mean_score = 0.\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "        states = env_info.vector_observations[0] if single_agent else env_info.vector_observations\n",
    "        agent.reset()                                                # reset the agent noise\n",
    "        score = 0 if single_agent else np.zeros(num_agents)\n",
    "\n",
    "        for _ in range(max_time_steps):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states, rewards, dones = step_tuple(env_info)\n",
    "            if single_agent:\n",
    "                agent.step(Experience(states, actions, rewards, next_states, dones))\n",
    "            else:\n",
    "                for idx in random.sample(range(num_agents), 10):\n",
    "                    agent.step(Experience(states[idx], actions[idx], rewards[idx], next_states[idx], dones[idx]))\n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        mean_score = np.mean(scores_deque)\n",
    "\n",
    "        print('\\r---{}---Episode {}\\tAverage Score: {:.2f}'.format(now,episode, mean_score), end=\"\")\n",
    "        if episode % 100 == 0:\n",
    "            print('\\r---{}---Episode {}\\tAverage Score: {:.2f}'.format(now,episode, mean_score))\n",
    "\n",
    "        stats[\"scores\"].append(score if single_agent else np.mean(score))\n",
    "        stats[\"episodes\"].append(episode)\n",
    "\n",
    "        if mean_score >= target:\n",
    "            print('\\n---{}---Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(now,episode, mean_score))\n",
    "            print(\"Target score of {0} has been reached. Saving model to {1}\".format(now, target, saved_model))\n",
    "            torch.save(agent.local_actor_network().state_dict(), saved_model)\n",
    "            break\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    print(now, \"- Finished training \" + \"successfully!\" if mean_score >= target else \"unsuccessfully!\")\n",
    "    return scores, stats\n",
    "\n",
    "def plot(stats):\n",
    "    scores = stats[\"scores\"]\n",
    "    episodes = stats[\"episodes\"]\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episode #\")\n",
    "    plt.plot(episodes, scores)\n",
    "    fig = plt.figure()\n",
    "    plt.show()\n",
    "\n",
    "def run(agent_: ContinuousControlAgent, env_: UnityEnvironment, num_episodes=2000, max_time_steps=1000, target=30.,\n",
    "        saved_model):\n",
    "\n",
    "        scores, stats= ddpg(agent_, env_, num_episodes=num_episodes, target=target, max_time_steps=max_time_steps,\n",
    "                        saved_model=saved_model)\n",
    "        plot(stats)\n",
    "        return scores, stats\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised 'Actor: Local' model\n",
      "Initialised 'Actor: Target' model\n",
      "Initialised 'Critic: Local' model\n",
      "Initialised 'Critic: Target' model\n",
      "2022-11-19 13:51:34.212906 - Training 20 agents for max 3000 episodes. Target score to reach is 30.0\n",
      "Agent experiences collected 32, agent requires  128 experiences\n",
      "Agent experiences collected 64, agent requires  128 experiences\n",
      "Agent experiences collected 96, agent requires  128 experiences\n",
      "Agent experiences collected 128, agent requires  128 experiences\n",
      "---2022-11-19 13:51:34.212906---Episode 69\tAverage Score: 30.07\n",
      "---2022-11-19 13:51:34.212906---Environment solved in 69 episodes!\tAverage Score: 30.07\n",
      "Target score of 2022-11-19 13:51:34.212906 has been reached. Saving model to 30.0\n",
      "2022-11-19 15:27:15.418694 - Finished training successfully!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3806f997f0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3804c94d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6bda09c60199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoint_single.pth'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msingle_agent\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'checkpoint_multi.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed=0)\n",
    "agent = ContinuousControlAgent(state_size, action_size, 0, memory=memory, update_frequency=2)\n",
    "stats = None\n",
    "saved_model = 'checkpoint_multi.pth'\n",
    "\n",
    "scores, stats = run(agent, env, num_episodes=3000, max_time_steps=1000, target=30., saved_model=saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
